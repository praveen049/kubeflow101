{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kubeflow adds kubectl to the provided notebook images, which allows developers to use notebooks to create and manage Kubernetes resources.The Jupyter notebook pods run under a special service account **default-editor**, which has namespace-scoped permissions to the following Kubernetes resources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME             SECRETS   AGE\n",
      "default          1         152d\n",
      "default-editor   1         152d\n",
      "default-viewer   1         152d\n"
     ]
    }
   ],
   "source": [
    "!kubectl get serviceaccounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                    READY   STATUS    RESTARTS   AGE\n",
      "achill-test-7-0         2/2     Running   0          12d\n",
      "praveen-poc-4g-pred-0   2/2     Running   0          6d22h\n",
      "test-2-0                2/2     Running   0          12d\n",
      "test-3-0                2/2     Running   0          12d\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distributed training jobs are managed by application-specific controllers, known as **operators**.These operators extend the Kubernetes APIs to \n",
    "create, manage, and manipulate the state of resources.For example, to run a distributed TensorFlow training job, the user just needs to provide a specification that describes the desired state (number of workers and parameter servers, etc.), and the TensorFlow operator component will take care of the rest and manage the life cycle of the training job.\n",
    "\n",
    "These operators allow the automation of important deployment concepts such as scalability, observability, and failover. They can also be used by pipelines to chain their execution with the execution of other components of the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
